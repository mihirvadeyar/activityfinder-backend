{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Temporal SFT Training (Colab)\n",
        "\n",
        "This notebook runs the full pipeline:\n",
        "1. Install dependencies\n",
        "2. Clone repo\n",
        "3. Upload `train.chat.jsonl` and `val.chat.jsonl`\n",
        "4. Train LoRA/QLoRA adapter\n",
        "5. Merge adapter\n",
        "6. Convert to GGUF\n",
        "7. Download GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install -U transformers trl datasets peft accelerate bitsandbytes sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!rm -rf activityfinder-backend\n",
        "!git clone https://github.com/mihir97/activityfinder-backend.git\n",
        "%cd /content/activityfinder-backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upload the two prepared files from your local repo:\n",
        "- `data/temporal/sft/train.chat.jsonl`\n",
        "- `data/temporal/sft/val.chat.jsonl`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p /content/data\n",
        "!cp -f /content/train.chat.jsonl /content/data/train.chat.jsonl\n",
        "!cp -f /content/val.chat.jsonl /content/data/val.chat.jsonl\n",
        "!ls -lh /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "TRAIN_FILE = \"/content/data/train.chat.jsonl\"\n",
        "VAL_FILE = \"/content/data/val.chat.jsonl\"\n",
        "ADAPTER_DIR = \"/content/temporal-qwen25-lora\"\n",
        "MERGED_DIR = \"/content/temporal-qwen25-merged\"\n",
        "OUT_DIR = \"/content/model-out\"\n",
        "GGUF_BASENAME = \"temporal-qwen25\"\n",
        "QUANT_TYPE = \"Q4_K_M\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/colab/train_temporal_sft.py \\\n",
        "  --base-model \"$BASE_MODEL\" \\\n",
        "  --train-file \"$TRAIN_FILE\" \\\n",
        "  --val-file \"$VAL_FILE\" \\\n",
        "  --output-dir \"$ADAPTER_DIR\" \\\n",
        "  --use-4bit \\\n",
        "  --epochs 8 \\\n",
        "  --batch-size 2 \\\n",
        "  --grad-accum 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/colab/merge_temporal_adapter.py \\\n",
        "  --base-model \"$BASE_MODEL\" \\\n",
        "  --adapter-dir \"$ADAPTER_DIR\" \\\n",
        "  --output-dir \"$MERGED_DIR\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!rm -rf llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "%cd /content/llama.cpp\n",
        "!cmake -B build\n",
        "!cmake --build build --config Release -j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/activityfinder-backend\n",
        "!LLAMA_CPP_DIR=/content/llama.cpp \\\n",
        "HF_MODEL_DIR=\"$MERGED_DIR\" \\\n",
        "OUT_DIR=\"$OUT_DIR\" \\\n",
        "GGUF_BASENAME=\"$GGUF_BASENAME\" \\\n",
        "QUANT_TYPE=\"$QUANT_TYPE\" \\\n",
        "bash scripts/colab/convert_to_gguf.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls -lh \"$OUT_DIR\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "ggufs = sorted(glob.glob(f\"{OUT_DIR}/*.gguf\"))\n",
        "print(\"GGUF files:\", ggufs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the quantized GGUF (usually ends with q4_k_m.gguf)\n",
        "target = [x for x in ggufs if \"q4_k_m\" in x.lower()]\n",
        "if not target:\n",
        "    target = ggufs\n",
        "if not target:\n",
        "    raise RuntimeError(\"No GGUF file found\")\n",
        "files.download(target[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After download, on your local machine run:\n",
        "\n",
        "```powershell\n",
        ".\\scripts\\importTemporalModel.ps1 -GgufPath C:\\\\path\\\\to\\\\temporal-qwen25.q4_k_m.gguf -ModelName qwen-temporal\n",
        "npm run predict:temporal-test -- --model qwen-temporal\n",
        "npm run eval:temporal-test\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
